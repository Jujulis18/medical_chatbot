import streamlit as st
from typing import Dict, List
import time
import sys
import os

# Ajouter le chemin pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.chatbot.rag_pipeline import RAGPipeline
from interface.utils.session_state import update_metrics, add_query_to_history

class ChatInterface:
    def __init__(self, settings: Dict):
        self.settings = settings
        self.rag_pipeline = self._initialize_rag_pipeline()
    
    def _initialize_rag_pipeline(self) -> RAGPipeline:
        """Initialiser le pipeline RAG avec les param√®tres"""
        try:
            print("Initialisation du pipeline RAG...")
            pipeline = RAGPipeline.from_config(self.settings)
            print("Pipeline RAG initialis√© avec succ√®s")
            return pipeline
        except Exception as e:
            st.error(f"Erreur lors de l'initialisation du pipeline RAG: {str(e)}")
            print(f"Erreur pipeline RAG: {e}")
            return None
    
    def render(self):
        """Afficher l'interface de chat"""
        st.title("üè• Assistant M√©dical RAG")
        st.markdown("*Assistant d'information m√©dicale - √Ä des fins √©ducatives uniquement*")
        
        # V√©rifier si le pipeline est initialis√©
        if self.rag_pipeline is None:
            st.error("‚ùå Le syst√®me n'est pas correctement initialis√©. Veuillez v√©rifier la configuration.")
            return
        
        # Container pour les messages
        messages_container = st.container()
        
        # Afficher l'historique des messages
        with messages_container:
            self._display_chat_history()
        
        # Input pour la nouvelle question
        self._render_chat_input()
        
        # Sidebar avec informations
        with st.sidebar:
            self._render_response_info()
            self._render_chat_controls()
    
    def _display_chat_history(self):
        """Affiche l'historique des conversations"""
        if not st.session_state.messages:
            return
        
        for i, message in enumerate(st.session_state.messages):
            with st.chat_message(message["role"]):
                st.write(message["content"])
                
                # Afficher les sources si disponibles et activ√©es
                if (message["role"] == "assistant" and 
                    "sources" in message and 
                    message["sources"] and
                    st.session_state.get("show_sources", True)):
                    
                    with st.expander(f"üìö Sources utilis√©es ({len(message['sources'])})"):
                        for j, source in enumerate(message["sources"]):
                            st.markdown(f"**Source {j+1}: {source.get('title', 'Sans titre')}**")
                            st.markdown(f"*Score de pertinence: {source.get('score', 0):.3f}*")
                            
                            # Afficher un extrait du contenu
                            content = source.get('content', '')
                            if len(content) > 200:
                                content = content[:200] + "..."
                            st.markdown(f"*Extrait:* {content}")
                            st.divider()
    
    def _render_chat_input(self):
        """G√©rer l'input de chat et les r√©ponses"""
        # Input de chat
        prompt = st.chat_input("üí¨ Posez votre question m√©dicale...")
        
        if prompt:
            self._process_user_question(prompt)
    
    def _process_user_question(self, prompt: str):
        """Traiter une question utilisateur"""
        # Ajouter la question de l'utilisateur √† l'historique
        st.session_state.messages.append({
            "role": "user", 
            "content": prompt
        })
        
        # Afficher la question utilisateur imm√©diatement
        with st.chat_message("user"):
            st.write(prompt)
        
        # G√©n√©rer et afficher la r√©ponse
        with st.chat_message("assistant"):
            response_container = st.empty()
            
            # Afficher un spinner pendant le traitement
            with st.spinner("üîç Recherche d'informations m√©dicales..."):
                start_time = time.time()
                
                try:
                    # Appeler le pipeline RAG
                    response_data = self.rag_pipeline.query(prompt)
                    response_time = time.time() - start_time
                    
                    # V√©rifier la structure de la r√©ponse
                    if isinstance(response_data, dict):
                        answer = response_data.get("answer", "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse.")
                        sources = response_data.get("sources", [])
                    else:
                        # Fallback si la r√©ponse n'est pas un dictionnaire
                        answer = str(response_data)
                        sources = []
                    
                    success = True
                    
                except Exception as e:
                    answer = f"‚ùå Une erreur s'est produite: {str(e)}\n\nPour des informations m√©dicales fiables, veuillez consulter un professionnel de sant√©."
                    sources = []
                    response_time = time.time() - start_time
                    success = False
                    
                    # Log l'erreur pour le d√©bogage
                    print(f"Erreur lors du traitement de la question: {e}")
            
            # Afficher la r√©ponse
            response_container.write(answer)
            
            # Afficher les sources si disponibles
            if sources and st.session_state.get("show_sources", True):
                with st.expander(f"üìö Sources consult√©es ({len(sources)})"):
                    for i, source in enumerate(sources):
                        st.markdown(f"**üìÑ {source.get('title', f'Document {i+1}')}**")
                        st.markdown(f"*Pertinence: {source.get('score', 0):.1%}*")
                        
                        # Extrait du contenu
                        content = source.get('content', '')
                        if content:
                            if len(content) > 300:
                                content = content[:300] + "..."
                            st.markdown(f"```\n{content}\n```")
                        
                        if i < len(sources) - 1:
                            st.divider()
        
        # Sauvegarder la r√©ponse dans l'historique
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources
        })
        
        # Mettre √† jour les m√©triques
        update_metrics(success, response_time)
        add_query_to_history(prompt, answer, sources, success)
        
        # Forcer le rafra√Æchissement pour afficher les nouveaux messages
        st.rerun()
    
    def _render_response_info(self):
        """Afficher les informations sur la derni√®re r√©ponse"""
        st.subheader("üìä Informations")
        
        # M√©triques g√©n√©rales
        metrics = st.session_state.get("metrics", {})
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Questions pos√©es", metrics.get("total_queries", 0))
        with col2:
            st.metric("R√©ponses r√©ussies", metrics.get("successful_queries", 0))
        
        if metrics.get("avg_response_time", 0) > 0:
            st.metric("Temps moyen", f"{metrics['avg_response_time']:.1f}s")
        
        # Informations sur la derni√®re r√©ponse
        if st.session_state.messages and len(st.session_state.messages) > 1:
            last_response = st.session_state.messages[-1]
            if last_response["role"] == "assistant" and "sources" in last_response:
                sources = last_response["sources"]
                if sources:
                    st.markdown("### üìö Derni√®re recherche")
                    st.metric("Sources trouv√©es", len(sources))
                    
                    # Graphique des scores de pertinence
                    if len(sources) > 1:
                        scores = [s.get("score", 0) for s in sources]
                        if any(score > 0 for score in scores):
                            st.bar_chart({"Pertinence": scores})
    
    def _render_chat_controls(self):
        """Afficher les contr√¥les du chat"""
        st.subheader("‚öôÔ∏è Contr√¥les")
        
        # Toggle pour afficher les sources
        show_sources = st.checkbox(
            "Afficher les sources", 
            value=st.session_state.get("show_sources", True),
            help="Afficher ou masquer les sources utilis√©es pour g√©n√©rer les r√©ponses"
        )
        st.session_state.show_sources = show_sources
        
        # Toggle pour le mode debug
        show_debug = st.checkbox(
            "Mode debug", 
            value=st.session_state.get("show_debug", False),
            help="Afficher des informations de d√©bogage"
        )
        st.session_state.show_debug = show_debug
        
        # Bouton pour effacer l'historique
        if st.button("üóëÔ∏è Effacer l'historique", help="Supprimer tous les messages de la conversation"):
            self._clear_chat_history()
        
        # Bouton pour exporter l'historique
        if st.button("üíæ Exporter la conversation", help="T√©l√©charger l'historique des messages"):
            self._export_chat_history()
        
        # Afficher les informations de debug si activ√©
        if show_debug:
            self._render_debug_info()
    
    def _clear_chat_history(self):
        """Effacer l'historique des messages"""
        from interface.utils.session_state import clear_chat_history
        clear_chat_history()
        st.success("‚úÖ Historique effac√©!")
        st.rerun()
    
    def _export_chat_history(self):
        """Exporter l'historique des messages"""
        try:
            from interface.utils.session_state import export_chat_history
            export_data = export_chat_history()
            
            st.download_button(
                label="üì• T√©l√©charger la conversation",
                data=export_data,
                file_name=f"conversation_medicale_{int(time.time())}.json",
                mime="application/json",
                help="T√©l√©charger la conversation au format JSON"
            )
        except Exception as e:
            st.error(f"Erreur lors de l'export: {str(e)}")
    
    def _render_debug_info(self):
        """Afficher les informations de d√©bogage"""
        with st.expander("üîß Informations de d√©bogage"):
            debug_info = {
                "Pipeline initialis√©": self.rag_pipeline is not None,
                "Nombre de messages": len(st.session_state.messages),
                "Param√®tres actuels": self.settings,
                "√âtat du syst√®me": "Op√©rationnel" if self.rag_pipeline else "Erreur"
            }
            
            for key, value in debug_info.items():
                st.text(f"{key}: {value}")
    
    def render_welcome_message(self):
        """Afficher un message d'accueil si c'est la premi√®re visite"""
        if len(st.session_state.messages) <= 1:  # Seulement le message initial
            st.info("""
            üëã **Bienvenue dans votre Assistant M√©dical RAG !**
            
            Je peux vous aider avec des informations m√©dicales g√©n√©rales sur :
            - Les sympt√¥mes de diverses pathologies
            - Les traitements g√©n√©raux
            - Les mesures de pr√©vention
            - Les informations de sant√© publique
            
            ‚ö†Ô∏è **Important**: Ces informations sont fournies √† titre √©ducatif uniquement. 
            Pour un diagnostic ou un traitement personnalis√©, consultez toujours un professionnel de sant√© qualifi√©.
            """)

# Test de l'interface
if __name__ == "__main__":
    print("Test de ChatInterface...")
    
    # Configuration de test
    test_settings = {
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "top_k": 3,
        "similarity_threshold": 0.5,
        "temperature": 0.7,
        "max_tokens": 500
    }
    
    # Cr√©er l'interface (n√©cessite Streamlit pour fonctionner compl√®tement)
    try:
        interface = ChatInterface(test_settings)
        print("‚úì Interface cr√©√©e avec succ√®s")
        
        if interface.rag_pipeline:
            print("‚úì Pipeline RAG initialis√©")
        else:
            print("‚ùå Probl√®me avec le pipeline RAG")
            
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        
    print("Test termin√©.")